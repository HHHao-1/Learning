#  机器学习与深度学习

- 人工智能

**人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门技术科学。**

- 机器学习 [机器学习.md](机器学习.md) 

**机器学习就是用算法解析数据，不断学习，对现实世界中发生的事做出判断和预测的一项技术。**

- 深度学习

深度学习是一种特殊的机器学习，它可以获得高性能也十分灵活。它可以用概念组成的网状层级结构来表示这个世界。**深度学习是用于建立、模拟人脑进行分析学习的神经网络，并模仿人脑的机制来解释数据的一种机器学习技术。**

- 总结

**机器学习是人工智能的一个子集，深度学习又是机器学习的一个子集。机器学习与深度学习都是需要大量数据来“喂”的，是大数据技术上的一个应用，同时深度学习还需要更高的运算能力支撑，如GPU。**

- 区别：
  - 深度学习是 data driven 的，需要大量的数据，而传统的机器学习算法通常不需要；
  - 深度学习本质上可以看作一个特征学习器，在无需另构特征情况下，传统的机器学习算法已经能够胜任日常的任务；
  - 如无必要，勿增实体。能够简单的模型解决的，不必要上深度学习算法。

# 算法分类

机器学习算法分为四类：监督式、无监督、半监督、强化学习

**1、监督式学习（Supervised learning）**

> 找输入输出间的映射

监督式学习是拥有一个输入变量（自变量）和一个输出变量（因变量），使用某种算法去学习从输入到输出之间的映射函数。目标是得到足够好的近似映射函数，当输入新的变量时可以以此预测输出变量。因为算法从数据集学习的过程可以被看作一名教师在监督学习，所以称为监督式学习。监督式学习可以进一步分为分类（输出类别标签）和回归（输出连续值）问题。

**2、非监督式学习（Unsupervised learning）**

> 在输入中找数据间的关系、结构

非监督式学习指的是只有输入变量，没有相关的输出变量。目标是对数据中潜在的结构和分布建模，以便对数据做进一步的学习。相比于监督式学习，非监督式没有确切的答案和学习过程也没有监督，算法独自运行发现和表达数据中的结构。非监督式学习进一步可以分为聚类问题（在数据中发现内在的分组）和关联问题（数据的各部分之间的关联和规则）。

**3、半监督式学习（Semi-Supervised Learning，SSL）**

> 非监督 + 监督

半监督式学习是一种监督式学习与非监督式学习相结合的一种学习方法。拥有大部分的输入数据（自变量）和少部分的有标签数据（因变量）。可以使用非监督式学习发现和学习输入变量的结构；使用监督式学习技术对无标签的数据进行标签的预测，并将这些数据传递给监督式学习算法作为训练数据，然后使用这个模型在新的数据上进行预测。

**4、强化学习（reinforcement learning）**

> 找最优决策

强化学习可以训练程序作出某一决定。程序在某一情况下尝试所有可能的行动，记录不同行动的结果并试着找出最好的一次尝试来做决定。是多学科多领域交叉的一个产物，它的本质是解决 **decision making** 问题，即自动进行决策，并且可以做连续决策。它主要包含四个元素，**agent，环境状态，行动，奖励**, 强化学习的目标就是获得最多的累计奖励。

# 准备工作

## 导数

### 概念

- 左右导数存在且相等是可导的充分必要条件。

- 可导必定连续。（如果确定一点，那么就知道这一点之后的走向，不会有突变）

- 连续不一定可导。（凸曲线顶点，左右导数不等）

### 几何意义

函数y=f（x）在x0点的导数f'（x~0~）的几何意义：表示函数曲线在点P~0~（x~0~,f（x~0~））处的切线的斜率（导数的几何意义是该函数曲线在这一点上的切线斜率）。

### 全导数

全导数本质上就是一元函数的导数。他是针对复合函数而言的定义。比如z=f(x,y)，x=u(t)，y=v(t)。那么z关于t的导数就是全导数。

### 偏导数

定义：一个多变量的函数的偏导数是它关于其中一个变量的导数，而保持其他变量恒定（相对于全导数，在其中所有变量都允许变化）。

数学表示：函数![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkixruwp5mj300a00i05w.jpg)关于变量x的偏导数写为![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkixry5zbdj300h00m07i.jpg)或![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkixrmolguj300o0170bk.jpg)。

几何含义：偏导数f'~x~(x~0~,y~0~)表示固定面上一点对x轴的切线斜率；偏导数f'~y~(x~0~,y~0~)表示固定面上一点对y轴的切线斜率。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkixwv8fizj30db0d5gm1.jpg" alt="img" style="zoom:50%;" />

例子：求 z=x^2^+3xy+y^2^ 在P（1,2）处的偏导数。

解：f'~x~ = 2x + 3y,  f'~y~ = 3x + 2y.

将（1,2）带入得

![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkixzrjaq3g301100i06l.gif) = 8， ![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkixzw4ipug301000k06z.gif) = 7

### 复合函数

已知函数y=f(u)，当u表示为u=g(x)时,y作为x的函数就可以表示为y=f(g(x))这样的嵌套结构，这种嵌套结构的函数，就称为f(u)、g(x)的复合函数。

![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkiz0s4atcj30ix0jnwfg.jpg)

### 链式法则

- **单变量函数链式法则**

已知单变量函数y=f(u)，当uu表示为单变量函数u=g(x)时，复合函数f(g(x))的导函数可以如下简单地求出来。
$$
\frac{dy}{dx} = \frac{dy}{du}\frac{du}{dx}
$$
上面这个公式称为单变量函数的复合函数求导公式，也称为**链式法则。**

法则：复合函数的导数可以像分数一样使用约分。但是这个约分的法则不适用于dx、dy的平方等情形。

![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkiyxwcfsmj30j507u0sz.jpg)



- **多变量函数链式法则**

![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkiz0kqp4nj30jz0art96.jpg)

**z关于y求导时，也是如此；在三个以上的变量的情况下也同样成立**

![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkiz1jdjtfj30jr09174m.jpg)

## 标准偏差

统计学名词，一种度量数据分布的分散程度的标准。

用以衡量数据值偏离算术平均值的程度。标准偏差越小，这些值偏离平均值就越少，反之亦然。标准偏差的大小可通过标准偏差与平均值的倍率关系来衡量。

##  S型函数

S型函数（Sigmoid function）是（back propagation，反向传播）BP神经网络中常用的非线性作用函数，即sigmoid函数，公式是$ f(x)=1/(1+e^{-x}) $。Sigmoid函数又分为Log-Sigmoid函数和Tan-Sigmoid函数。

## 矩阵

在线性代数中，当两个矩阵相乘时，第一个矩阵的*列*数必须等于第二个矩阵的*行*数。

- `3x4` 矩阵乘以 `4x2` 矩阵是**有效**的，可以得出一个 `3x2` 矩阵。
- `4x2` 矩阵乘以 `3x4` 矩阵是**无效**的。

![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkipd0dlmaj30ci034q2q.jpg)

![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkipd0pgm8j30ci044glf.jpg)

![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkipd5wp75j30ci031wea.jpg)



### 点积

点乘一般就是指点积。

点积在数学中，又称数量积（dot product; scalar product），是指接受在实数R上的两个向量并返回一个实数值标量的二元运算。

> 两个向量a = [a1, a2,…, an]和b = [b1, b2,…, bn]的点积定义为：
>
> a·b=a1b1+a2b2+……+anbn。
>
> 使用矩阵乘法并把向量当作n×1 矩阵，点积还可以写为：
>
> a·b=（a^T^）* b，这里的a^T^指示矩阵a的转置。

## 张量

### 概念

张量是任意维度的数组

- **标量**是零维数组（零阶张量）。例如，`5`
- **矢量**是一维数组（一阶张量）。例如，`[2, 3, 5, 7, 11]` 或 `[5]`
- **矩阵**是二维数组（二阶张量）。例如，`[[3.1, 8.2, 5.9][4.3, -2.7, 6.5]]`

### 形状

输出张量不仅会返回其值，还会返回其形状以及存储在张量中的值的类型。

形状用于描述张量维度的大小和数量。张量的形状表示为 `list`，其中第 `i` 个元素表示维度 `i` 的大小。列表的长度表示张量的阶（即维数）。

```js
tf.tensor4d([[[[1], [2]], [[3], [4]]]]) 
......
dtype: "float32"
shape: (4) [1, 2, 2, 1]
```

### 广播

在数学中，您只能对形状相同的张量执行元素级运算（例如，相加和等于）。

在 TensorFlow 中，您可以对张量执行传统意义上不可行的运算。

TensorFlow 支持**广播**（一种借鉴自 NumPy 的概念）。利用广播，元素级运算中的较小数组会增大到与较大数组具有相同的形状。

- 如果运算需要大小为 `[6]` 的张量，则大小为 `[1]` 或 `[]` 的张量可以作为运算数。
- 如果运算需要大小为 `[4, 6]` 的张量，则以下任何大小的张量都可以作为运算数：
  - `[1, 6]`
  - `[6]`
  - `[]`
  - 如：[1,6] + [2] = [1+2, 6+2]

------

## TensorFlow

### 简介

- 能做什么？

> 运行现有模型
>
> 重新训练现有模型
>
> 使用Javastript开发机器学习模型

### 指令

> 创建、销毁和操控张量。

典型 TensorFlow 程序中的大多数代码行都是指令。

###  图

> （也称为**计算图**或**数据流图**）是一种图数据结构。很多 TensorFlow 程序由单个图构成，但是 TensorFlow 程序可以选择创建多个图。
>
> 图的节点是指令，图的边是张量。张量流经图，在每个节点由一个指令操控。一个指令的输出张量通常会变成后续指令的输入张量。TensorFlow 会实现**延迟执行模型**，意味着系统仅会根据相关节点的需求在需要时计算节点。
>
> 张量可以作为**常量**或**变量**存储在图中。常量存储的是值不会发生更改的张量，而变量存储的是值会发生更改的张量。常量和变量都只是图中的一种指令。常量是始终会返回同一张量值的指令，变量是会返回分配给它的任何张量的指令。
>

常量：

```js
  x = tf.constant([5.2])
```

变量：

```js
  y = tf.Variable([5])
```

给变量分配值（注意：您始终需要指定一个默认值）：

```js
  y = tf.Variable([0])
  y = y.assign([5])
```

定义一些常量或变量后，您可以将它们与其他指令（如 `tf.add`）结合使用。 `tf.add` 指令它会调用您的 `tf.constant` 或 `tf.Variable` 指令，以获取它们的值，然后返回一个包含这些值之和的新张量。

### 会话

> 图必须在 TensorFlow **会话**中运行，会话存储了它所运行的图的状态
>

```js
with tf.Session() as sess:
  initialization = tf.global_variables_initializer()
  print(y.eval())
```

在使用 `tf.Variable` 时，您必须在会话开始时调用 `tf.global_variables_initializer`，以明确初始化这些变量。

**注意：**会话可以将图分发到多个机器上执行（假设程序在某个分布式计算框架上运行）。有关详情，请参阅[分布式 TensorFlow](https://www.tensorflow.org/deploy/distributed)。

### 总结

TensorFlow 编程本质上是一个两步流程：

1. 将常量、变量和指令整合到一个图中。
2. 在一个会话中执行这些常量、变量和指令。

# 主要术语

> （监督式）机器学习定义：
>
> - 机器学习系统通过学习如何组合输入信息来对从未见过的数据做出有用的预测。

## 标签

**标签**是我们要预测的事物，即简单线性回归中的 `y` 变量。标签可以是小麦未来的价格、图片中显示的动物品种、音频剪辑的含义或任何事物。

## 特征

**特征**是输入变量，即简单线性回归中的 `x` 变量。简单的机器学习项目可能会使用单个特征，而比较复杂的机器学习项目可能会使用数百万个特征，按这样的方式指定： x~1~,x~2~,...x~N~​

> 在垃圾邮件检测器示例中，特征可能包括：
>
> - 电子邮件文本中的字词
> - 发件人的地址
> - 发送电子邮件的时段
> - 电子邮件中包含“一种奇怪的把戏”这样的短语。

## 样本

**样本**是指数据的特定实例：**x**。（我们采用粗体 **x** 表示它是一个矢量。）我们将样本分为以下两类：

- 有标签样本
- 无标签样本

**有标签样本**同时包含特征和标签。即：

```
  labeled examples: {features, label}: (x, y)
```

我们使用有标签样本来**训练**模型。在我们的垃圾邮件检测器示例中，有标签样本是用户明确标记为“垃圾邮件”或“非垃圾邮件”的各个电子邮件。

**无标签样本**包含特征，但不包含标签。即：

```
  unlabeled examples: {features, ?}: (x, ?)
```

在使用有标签样本训练模型之后，我们会使用该模型预测无标签样本的标签。在垃圾邮件检测器示例中，无标签样本是用户尚未添加标签的新电子邮件。

## 模型

模型定义了特征与标签之间的映射关系。例如，垃圾邮件检测模型可能会将某些特征与“垃圾邮件”紧密联系起来。我们来重点介绍一下模型生命周期的两个阶段：

- **训练**：指创建或**学习**模型。也就是说，向模型展示有标签样本，让模型逐渐学习特征与标签间的关系。
- **推断**：指将训练后的模型应用于无标签样本。也就是说，使用经过训练的模型做出有用的预测 (`y'`)。

## 回归

**回归**模型可预测连续值。例如，回归模型做出的预测可回答如下问题：

- 今年、明年加利福尼亚州一栋房产的价值是多少？
- 今天、明天用户点击此广告的概率是多少？

## 分类

**分类**模型可预测离散值。例如，分类模型做出的预测可回答如下问题：

- 某个指定电子邮件是垃圾邮件还是非垃圾邮件？
- 这是一张狗、猫还是仓鼠的图片？

## 补充

1. 有些标签可能不可靠。

   如：有些垃圾内容发布者或僵尸网络可能会故意提供错误标签来误导我们的模型。

2. 合适的特征应该是具体且可量化的。

   主题标头中的字词可能是优质特征，但不适合做标签。

   美观程度是一种过于模糊的概念，不能作为实用特征。美观程度可能是某些具体特征（例如样式和颜色）的综合表现。样式和颜色都比美观程度更适合用作特征。

   喜好不是可观察且可量化的指标。我们能做到最好的就是针对用户的喜好来搜索可观察的代理指标。

   用户可能只是想要详细了解他们喜欢的鞋子。因此，用户点击次数是可观察且可量化的指标，可用来训练合适的标签。

# TensorFlow.js

## 安装

- 使用脚本标签

搜索 `tensorflow js setup`

```html
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0/dist/tf.min.js"></script>
```

- 使用npm或yarn

```js
yarn add @tensorflow/tfjs

npm install @tensorflow/tfjs
```

- 使用node

```js
1: 安装带有原生C++绑定的TensorFlow.js。

yarn add @tensorflow/tfjs-node
npm install @tensorflow/tfjs-node

2: （仅限Linux）如果您的系统具有支持CUDA的NVIDIA®GPU，请使用GPU包以获得更高的性能。

yarn add @tensorflow/tfjs-node-gpu
npm install @tensorflow/tfjs-node-gpu

3: 安装纯JavaScript版本，这是性能方面最慢的选项。

yarn add @tensorflow/tfjs
npm install @tensorflow/tfjs
```

## 调用

- Parcel

使用Parcel或Webpack构建

Parcel：快速，零配置的Web应用程序打包程序

```js
安装：
yarn global add parcel-bundler
npm install -g parcel-bundler

使用以下命令在项目目录中创建package.json文件：
yarn init -y
npm init -y

启动：
parcel index.html -p 2345 //不间端口默认1234
parcel watch index.html 	//使用自己的服务器
```

- Node

```js
const tf = require('@tensorflow/tfjs-node')

-> node index(.js 可省略) //命令行执行
```

## 张量

tensor 是 TensorFlow.js 的数据中心单元：由一组数值组成的一维或多维数组。

Tensor 实例的 shape 属性定义了这个数组的形状。

在 TensorFlow.js 中，张量值是不可改变的；一旦创建，你就不能改变它的值。相反，如果你执行 operations（ops） 操作，就可以生成新的张量值。

### 构建

下述 张量的表示方法，除了 scalar 和 tensor1d 两方法没有 shape 属性外，其它的都可传入values、shape、dtype 三个参数。1d、2d等可以省略。

```js
// 0阶张量，即标量
tf.scalar(3.14).print(); // 3.140000104904175， 默认dtype 是 float32
tf.scalar(3.14, 'float32').print(); // 3.140000104904175
tf.scalar(3.14, 'int32').print(); // 3
tf.scalar(3.14, 'bool').print(); // 1

// 1阶张量
tf.tensor1d([1, 2, 3]).print(); // [1, 2, 3]

// 2阶张量
// Pass a nested array.
tf.tensor2d([[1, 2], [3, 4]]).print();
// Pass a flat array and specify a shape.
tf.tensor2d([1, 2, 3, 4], [2, 2]).print();
// ouput
// [[1, 2],[3, 4]] 

// 3阶张量
// Pass a nested array.
tf.tensor3d([[[1], [2]], [[3], [4]]]).print();
// 2x3 Tensor
const shape = [2, 3]; // 可以看做是两行三列组成
const a = tf.tensor([1.0, 2.0, 3.0, 10.0, 20.0, 30.0], shape);
// Pass a flat array and specify a shape.
tf.tensor3d([1, 2, 3, 4], [2, 2, 1]).print();
// output
//[[[1],[2]],[[3],[4]]] 

// 4阶张量
// Pass a nested array.
tf.tensor4d([[[[1], [2]], [[3], [4]]]]).print();
// Pass a flat array and specify a shape.
tf.tensor4d([1, 2, 3, 4], [1, 2, 2, 1]).print();
// output
// [[[[1],[2]],[[3],[4]]]] 

// 把 Tensor 实例中的所有元素的值重置为 0 和 1 
const zeros = tf.zeros([3, 5]);//3行5列
// Output
// [[0, 0, 0, 0, 0],[0, 0, 0, 0, 0],[0, 0, 0, 0, 0]]              

tf.ones([2, 2]).print();//2行2列
// output 
// [[1, 1],[1, 1]] 
```

### 变量

variables 是用一个张量值初始化的。不像张量（tensor），值不可改变。你可以使用 assign 方法给一个存在的变量（variable）分配一个新的张量：

```js
const initialValues = tf.zeros([5]);
const biases = tf.variable(initialValues); // 初始化偏差（距离原点的截距或偏移）
biases.print(); // output: [0, 0, 0, 0, 0]

const updatedValues = tf.tensor1d([0, 1, 0, 1, 0]);
biases.assign(updatedValues); // update values of biases
biases.print(); // output: [0, 1, 0, 1, 0]
```

变量（variable）主要是用于在模型训练时，进行数据的保存和更新。

### 操作

tensors 可以用来保存数据，而 operations（ops） 可以操作数据。TensorFlow.js 提供了多种适用于张量的线性代数和机器学习的运算的 operations。由于张量是不可改变的，所以 operations 操作并不会改变 tensors 的值，而是返回新的张量。

- operations 提供了类似 square 等一元运算：

```js
const x = tf.tensor1d([1, 2, Math.sqrt(2), -1]);
x.square().print();  // or tf.square(x)
// [1, 4, 1.9999999, 1]

const x = tf.tensor1d([1, 2, 4, -1]);
x.sqrt().print();  // or tf.sqrt(x)
// [1, 1.4142135, 2, NaN]
```

- operations 提供了类似 add、sub 等二元运算：

```js
const a = tf.tensor1d([1, 2, 3, 4]);
const b = tf.tensor1d([10, 20, 30, 40]);

a.add(b).print();  // or tf.add(a, b)
// [11, 22, 33, 44]
```

- 支持链式操作：

```js
const e = tf.tensor2d([[1.0, 2.0], [3.0, 4.0]]);
const f = tf.tensor2d([[5.0, 6.0], [7.0, 8.0]])
const sq_sum = e.add(f).square();
sq_sum.print();
// Output: [[36 , 64 ],
//          [100, 144]]

// 所有的操作都暴露在函数的命名空间中，也可以进行下面操作，得到相同的结果
const sq_sum = tf.square(tf.add(e, f));
```

## 模型和层

从概念上看，一个模型就是一个函数，给定相应输入得到期望的输出。有两种创建模型的方式：

- Layers API

- Core API（如 tf.matMul() 或 tf.add() 等）

#### Layers API

Layers API有两种方式创建模型：

第一种是创建 sequential 模型

第二种是创建 functional 模型

##### sequential model

最常见的模型是 Sequential 模型。Sequential 模型将网络的每一层简单的叠在一起。您可以将需要的层按顺序写在一个列表里，然后将列表作为 sequential() 函数的输入：

```js
const model = tf.sequential({
 layers: [
   tf.layers.dense({inputShape: [784], units: 32, activation: 'relu'}),
   tf.layers.dense({units: 10, activation: 'softmax'}),
 ]
});
```

或用 add() 方法：

```js
const model = tf.sequential();
model.add(tf.layers.dense({inputShape: [784], units: 32, activation: 'relu'}));
model.add(tf.layers.dense({units: 10, activation: 'softmax'}));
```

> 注意：模型的第一层需要“输入形状”参数（inputShape）。不要在“输入型状”中包含 batch size（批次大小）。假设您要向模型输入一个形状为[B, 784]的张量（B 是任意batch size），您只需要将“输入型状”设为[784]。

您可以通过model.layers来使用模型中的每一层。例如，您可以用 model.inputLayers 和 model.outputLayers 来调用输入层和输出层。

##### functional model

我们也可以通过 tf.model() 来创建 LayersModel。tf.model() 和 tf.sequential() 的主要区别为，您可以用 tf.model() 来创建任何非闭环的计算图。

```js
// 用apply()方法创建任意计算图
const input = tf.input({shape: [784]});
const dense1 = tf.layers.dense({units: 32, activation: 'relu'}).apply(input);
const dense2 = tf.layers.dense({units: 10, activation: 'softmax'}).apply(dense1);
const model = tf.model({inputs: input, outputs: dense2});
```

我们在每一层用 apply() 将上一层的输出作为本层的输入。apply() 返回一个 SymbolicTensor（类似于张量，但不包含任何数值）。

不同于 sequential model 使用 inputShape 来定义第一层的输入，我们用 tf.input() 创建的 SymbolicTensor 作为第一层的输入。

如果您向 apply() 输入一个数值张量，它会进行计算并返还一个数值张量：

```js
const t = tf.tensor([-2, 1, 0, 5]);
const o = tf.layers.activation({activation: 'relu'}).apply(t);
o.print(); // [0, 1, 0, 5]
```

这个方式适用于单独测试每一层并检查它们的输出。

和 sequential model 一样，您可以通过 model.layers 来使用模型中的每一层。例如，您可以用 model.inputLayers 和 model.outputLayers 来调用输入层和输出层。

##### 验证

Sequential model和functional model都属于 LayersModel类。使用 LayersModels 让验证更方便：它要求您定义输入形状，并用您定义的形状来验证您对模型的输入。LayersModel 会自动计算模型中所有张量的形状。知道张量的形状后，模型就可以自动创建它所需要的参数。您也可以用形状信息来判断两层相邻的层是否相互兼容。

##### 模型总览

使用 model.summary() 可以显示很多模型的重要信息，包括：

- 每一层的名字和类型
- 每一层的输出形状
- 每一层的权重数量
- 每一层的输入
- 一个模型拥有的可训练参数总量，和不可训练参数总量

用前面定义的模型来做例子，我们可以在命令行中得到以下信息：

| Layer (type)                                                 | Output shape | Param # |
| ------------------------------------------------------------ | ------------ | ------- |
| dense_Dense1 (Dense)                                         | [null,32]    | 25120   |
| dense_Dense2 (Dense)                                         | [null,10]    | 330     |
| Total params: 25450 Trainable params: 25450 Non-trainable params: 0 |              |         |

注意：每一层的输出形状中都含有 null 值。模型的输入形状包含了批次大小，而批次大小是可以灵活更变的，所以批次的值在张量形状中以 null 显示。

##### 序列化

相对底端API而言，使用 LayersModel的另一个好处是方便存储、加载模型。LayersModel 包含如下信息：

- 可用于重建模型的模型架构信息
- 模型的权重
- 训练配置（例如损失函数，优化器和评估方式）
- 优化器的状态（可用于继续训练模型）

存储和加载模型只需要一行代码： 

```js
const saveResult = await model.save('localstorage://my-model-1');
const model = await tf.loadLayersModel('localstorage://my-model-1');
```

在这个例子中，模型被存储在浏览器的本地存储里。请访问 model.save() 和 save and load 了解如何把模型保存在不同的媒介中（例如 file storage, IndexedDB, 触发下载到浏览器等等）。

##### 自定义层

层是创建模型的基础。如果您的模型需要定制化计算模块，您可以写一个自定义层并插入模型中。下面的例子是一个计算平方和的自定义层：

```js
class SquaredSumLayer extends tf.layers.Layer {
 constructor() {
   super({});
 }
 // In this case, the output is a scalar.
 computeOutputShape(inputShape) { return []; }

 // call() is where we do the computation.
 call(input, kwargs) { return input.square().sum();}

 // Every layer needs a unique name.
 getClassName() { return 'SquaredSum'; }
}
```

可以用 apply() 方法在一个张量上测试这个自定义层

```js
const t = tf.tensor([-2, 1, 0, 5]);
const o = new SquaredSumLayer().apply(t);
o.print(); // prints 30
```

> 注意：如果您在模型中包含了自定义层，模型将不能序列化

#### Core API

本文开头提到了两种在 TensorFlow.js 中建立模型的方法。最常用的方式是使用 Layers API，因为它的模式是基于广泛应用的 Keras API（详情见 best practices and reduces cognitive load）。Layers API 提供了大量方便的工具，例如权重初始化，模型序列化，训练监测，可迁移性和安全检查。

当您遇到如下情况时，可能会需要使用 Core API：

- 您需要更多灵活性和控制
- 您不需要序列化或可以创造自己的序列化方法

用 Core API 写的模型包含了一系列的函数。这些函数以一个或多个张量作为输入，并输出另一个张量。我们可以用 Core API 来重写之前定义的模型：

```js
// The weights and biases for the two dense layers.
const w1 = tf.variable(tf.randomNormal([784, 32]));
const b1 = tf.variable(tf.randomNormal([32]));
const w2 = tf.variable(tf.randomNormal([32, 10]));
const b2 = tf.variable(tf.randomNormal([10]));

function model(x) {
  return x.matMul(w1).add(b1).relu().matMul(w2).add(b2).softmax();
}
```

在 Core API 中，我们需要自己创建和初始化权重。每个权重都是一个 Variable，TensorFlow.js 会把 Variable 权重设为可训练张量。您可以用 tf.variable() 创建 Variable 或把一个已存在的张量放到 Variable 中。

#### 示例

- 通过使用 core api 创建模型

*一个二元方程式求解的表示法*

```js
// Define function
function predict(input) {
  // y = a * x ^ 2 + b * x + c
  return tf.tidy(() => {
    const x = tf.scalar(input);

    const ax2 = a.mul(x.square());
    const bx = b.mul(x);
    const y = ax2.add(bx).add(c);

    return y;
  });
}

// Define constants: y = 2x^2 + 4x + 8
const a = tf.scalar(2);
const b = tf.scalar(4);
const c = tf.scalar(8);

// Predict output for input of 2
const result = predict(2);
result.print() // Output: 24
```

- 通过使用 layers api 创建模型

*简单的线性回归的定义*

```js
const model = tf.sequential();
model.add(
  tf.layers.simpleRNN({
    units: 20,
    recurrentInitializer: 'GlorotNormal',
    inputShape: [80, 4]
  })
);

const optimizer = tf.train.sgd(LEARNING_RATE);
model.compile({optimizer, loss: 'categoricalCrossentropy'});
model.fit({x: data, y: labels)});
```

> 上述例子通过创建一个线性回归模型，调用 simpleRNN 层，通过 sgd 优化算法训练，得到期望效果。
>
> 在 TensorFlow.js 中有很多不同类型的层（layers）表示法，例如 tf.layers.simpleRNN, tf.layers.gru, 和 tf.layers.lstm 等等。

## 内存管理

- tf.dispose 

- tf.tidy

由于，tensorFlow.js 使用了 GPU 加速数学运算，在使用张量和变量时，管理 GPU 的内存是必不可少的。 TensorFlow.js 提供了 dispose 和 tf.tidy 两个函数来帮助处理内存：

 1、dispose 

你可以调用一个张量或变量来**清除和释放它的 GPU 内存。**

```js
const x = tf.tensor2d([[0.0, 2.0], [4.0, 6.0]]);
const x_squared = x.square();

x.dispose();
x_squared.dispose();
```

2、tf.tidy 

在做大量张量操作时，使用 dispose 可能很笨重。TensorFlow.js 提供了另一个功能 tf.tidy。 tf.tidy 会执行一个功能，清除任何创建的中间张量，释放它们的 GPU 内存。但它不会清除内部函数的返回值。

```js
// tf.tidy takes a function to tidy up after
const average = tf.tidy(() => {

// tf.tidy 会清除在这个函数内的张量使用的所有GPU内存，而不是返回的张量。
// 即使在像下面这样的一个简短的操作序列中，也会创建一些中间的张量。所以，把 ops 放在 tidy 函数中是一个好的选择

  const y = tf.tensor1d([1.0, 2.0, 3.0, 4.0]);
  const z = tf.ones([4]);

  return y.sub(z).square().mean();
});

average.print() // Output: 3.5
```

使用 tf.tidy 将有助于防止应用程序中的内存泄漏。当内存被回收时，它也可以用来更小心地控制。

3、两个注意点

- 传递给 tf.tidy 函数是同步的，不会返回一个 Promise 对象。
- tf.tidy 不会清理变量。变量通常贯穿于机器学习模型的整个生命周期中，但是，你可以手动调用 dispose ( )。

## 记录

### slice

![image-20201115203746973](https://tva1.sinaimg.cn/large/0081Kckwly1gkq50vb3f0j30ap0aqjs0.jpg)

# 神经网络

## 概念

### 结构

- 加权输入公式：$$ z=w_1x_1+w_2x_2+w_3x_3+...+b $$
- 激活公式：$y=a(z)$
  - 用于添加一些非线性变化
    - 非线性变化：如年薪对相亲的影响刚开始是线性增长，当年新增长到一定程度（只有个钱多的概念），它的影响就不能再线性增长，线该变平了
    - 激活函数sigmoid作用：把一个连续输出的值压缩到0~1之间
- 神经元构造图：![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkj409wrpcj30xw09c3zf.jpg)

我们只要将这样的神经元连接成网络状，那么就形成了`神经网络`。这里说的是最基础的`阶层型神经网络`,还有`卷积神经网络`,`循环神经网络`等。

- 阶层型神经网络构造图：

![img](https://tva1.sinaimg.cn/large/0081Kckwly1gkj47xazrpj30zc0hotb3.jpg)

### 层级职责

神经网络通常包括一个输入层、若干隐藏层、一个输出层（输出层通常不用于计算神经网络的层数）

- `输入层`：该层负责读取神经网络需要的数据，这个层的神经元没有输入监听，他们的输出是将读取的数据原样输出。**输入==输出**

- `隐藏层`：该层神经元则负责上面我们所回顾的两个公式的计算操作，在神经网络中，这层是实际处理信息的部分。**计算z和y**

- `输出层`：该层与隐藏层一样，执行两个公式的计算，并且显示神经网络的计算结果，也就是最终输出。**计算z和y并显示结果**

### 示例

- 手写数字识别

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkj4d5h51yj310c0u0gq7.jpg" alt="img" style="zoom:50%;" />

## 知识点

### 训练

- 如何训练

给定大量输入和输出，算出神经网络里所有神经元的权重、偏置，然后给定新的输入,可以算出新的输出

在机器学习里输入输出被称为特征和标签,大量输入输出被称为训练集

- 训练步骤

初始化：随机生成一些权重和偏置（既随便先猜一个）

给定特征：计算出标签，得到它与真实标签差得多远

优化：微调权重和偏置,使损失变小

### 传播

前向传播：将训练数据的特征送入网络，得到标签

反向传播：计算损失并优化

### 损失

> 如何计算损失？
>
> 使用损失函数，如：均方误差、对数损失、交叉熵等

关于计算预测值和正解的误差总和的函数称为`代价函数`，符号表示是$C_T$（T是Total的首字母）

![两个直角坐标曲线图，每个曲线图显示一条线和一些数据点。在第一个曲线图中，线与数据极其不吻合，所以损失较大。在第二个曲线图中，线与数据比较吻合，所以损失较小。](https://tva1.sinaimg.cn/large/0081Kckwly1gkj5vo6uzij310c0c4jrc.jpg)

**图 . 左侧模型的损失较大；右侧模型的损失较小。**

**平方损失：**

利用平方误差确定参数的方法在数学上称为最小二乘法，它在统计学中是回归分析的常规手段。

单个样本的平方损失如下：

```mathematica
= the square of the difference between the label and the prediction
= (observation - prediction(x))2
= (y - y')2
```

**均方误差** (**MSE**)：

 指的是每个样本的平均平方损失。求出各个样本的所有平方损失之和，然后除以样本数量：
$$
MSE=\frac{1}{N}\sum_{(x,y)∈D}(y−prediction(x))^2
$$
其中：

- (x,y)指的是样本，其中
  - x 指的是模型进行预测时使用的特征集（例如，温度、年龄和交配成功率）。
  - y 指的是样本的标签（例如，每分钟的鸣叫次数）。
- prediction(x) 指的是权重和偏差与特征集 x 结合的函数。
- D 指的是包含多个有标签样本（即 (x,y)）的数据集。
- N 指的是 D 中的样本数量。

### 优化器

> 如何优化？
>
> 使用优化器，如：随机梯度下降(SGD)、Adam等

### 损失函数

#### 对数损失函数

#### 交叉熵损失函数

- 对数损失函数的多分类版本

### 激活函数

*代表一种添加非线性变化的能力*

- linear

> 激活函数相当于没设 激活函数

- sgd

- adam

> 可以自动调节学习率，很多场景下可以替代sgd

- relu

> 如果特征小于0输出0，否则输出输入值
>
> 用于去除小于0的特征
> $$
> f(x) = max(0,x)
> $$

### 拟合

- 欠拟合
  - 数据复杂，模型过于简单，loss始终降不下去
  - 如下：黑色的线十数据，蓝色的模型不能拟合数据

<img src="/Users/chenghao/Library/Application Support/typora-user-images/image-20201111175813477.png" alt="image-20201111175813477" style="zoom:33%;" />

- 过拟合

  - 数据简单、模型过于复杂
  - 噪音容易引起过拟合
  - 如下：二分类问题，黑线是正常拟合，绿线是过拟合

  <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkldvxneiij30lq0l87dy.jpg" alt="image-20201111175702576" style="zoom:33%;" />

### 过拟合应对法

> **早停法、权重衰减、丢弃法**
>
> 也可以增加训练集，但成本高

- 早停法：观察损失变化转折点，下次在这个位置停止训练

- 权重衰减：把权重复杂度也作为模型损失的一部分，过于复杂的权重在训练过程中被衰减掉（达到简化模型的目的），防止过拟合

```js
 // l2正则化就是权重衰减
// 这里1是正则化率，超参数
 kernelRegularizer: tf.regularizers.l2({ l2: 1 }) 
```

- 丢弃发：在隐藏层设置丢弃率，随机丢弃某几个神经元的权重（既相当于把隐藏层中神经元个数减少，达到简化模型的目的）

```js
// 丢弃法，这里0.9是丢弃率，既丢弃9个神经元权重
model.add(tf.layers.dropout({ rate: 0.9 }));
```



## 应用分类

### 线性回归问题

#### 概念

> y = b + mx

其中：

- y 指的是温度（以摄氏度表示），即我们试图预测的值。
- m 指的是直线的斜率。
- x 指的是每分钟的鸣叫声次数，即输入特征的值。
- b 指的是 y 轴截距。

按照机器学习的惯例，您需要写一个存在细微差别的模型方程式：

> $y^′ = b + w_1x_1~$

其中：

- y′ 指的是预测标签（理想输出值）。
- b 指的是偏差（y 轴截距）。而在一些机器学习文档中，它称为 w~0~。
- w~1~ 指的是特征 1 的权重。权重与上文中用 m 表示的“斜率”的概念相同。
- x~1~指的是特征（已知输入项）。

下标（例如 w~1~和 x~1~）预示着可以用多个特征来表示更复杂的模型。例如，具有三个特征的模型可以采用以下方程式：

> $y^′=b+w_1x_1+w_2x_2+w_3x_3~$

#### 操作步骤

- 加载线性数据集
- 定义神经网络模型
- 训练模型并预测

#### 归一化

> 把大数量级特征转化到较小的数量级下，通常是[0,1]或[-1,1]

- 为什么归一化？

绝大多数Tensorflow.js的模型都不是给特别大的数设计的

将不同数量级的特征转换到同一数量级,防止某个特征影响过大（如：不同的类型特征---房价、房屋面积）

- 操作步骤

定义一个神经网络模型

将归一化后的数据喂给模型学习 --- 训练模型

预测后，把结果反归一化为正常数据

### 二分类问题

#### 逻辑回归

> 既二分类问题
>
> 解决分类问题，返回一个概率（0~1）

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkl8ils1w1j30j80j2ajd.jpg" alt="image-20201111145104002" style="zoom:33%;" />

操作步骤

- 加载二分类数据集
- 定义带有激活函数的神经网络模型
- 训练模型并预测

#### XOR逻辑回归

异或逻辑回归（相同为0，不同为1）

图上的点：x坐标都为正或都为负是蓝色的点，一正一负为黄色的点

目的：分开黄色的点与蓝色的点

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkl6ky50pzj30iy0iy15c.jpg" alt="image-20201111134414296" style="zoom: 33%;" />

### 多分类问题

操作步骤

- 加载IRIS（鸢尾花）数据集 (**训练集与验证集**)
  - 验证集：用来验证模型是不是越训练越好了
    - 如：训练中训练集的损失越来越低了，而验证集的损失越来越高，这是就要考虑调整模型结构，调整超参数等
  - 验证集与数据集的格式是一模一样的
  - 验证集往往可以从训练集中分一部分出来
  - 与训练集的区别：神经网络的权重会因为训练集的数据而变化，但不会因为验证集的数据变化

- 定义模型结构
  - 带有softmax激活函数的多层神经网络
    - softmax：将输出压缩为和为1的多个数值（如：概率04，0.4，0.2）
- 训练模型并预测
  - 可视化损失、准确度

## 概念补充

### 全连接层

全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。

### 最优化

最优化是应用数学的一个分支，主要指在一定条件限制下，选取某种研究方案使目标达到最优的一种方法。

- 常见方法

1. 梯度下降法（Gradient Descent）

2. 牛顿法（Newton's Method）和拟牛顿法（Quasi-Newton Methods）

3. 共轭梯度法（Conjugate Gradient）

4. 启发式优化方法

5. 拉格朗日乘数法的基本思想

- 数学模型

最优化问题的共同特点是：求满足一定条件的变量x1，x2，…，xn，使某函数f(x1，x2，…，xn)取得最大值或者最小值。由于f(x1，x2，…，xn)的最大问题可以转化为-f(x1，x2，…，xn)的最小问题，所以较多时候只讨论最小问题。这里的函数f(x1，x2，…，xn)称为**目标函数**或者**评价函数**；变量x1，x2，…，xn称为决策变量；需要满足的条件称为约束条件；用以构成约束条件的函数称为约束函数。

# 卷积神经网络

*Convolutional Neural Networks, CNN*

## 为什么要用

- 图片数据量大,运算量大
  - 200 * 200 * 3= 120, 000（图片像素量非常大，200*200像素的图片就有12万个特征）
  - 普通神经网络效率太低，而且特征一多网络一复杂，很容易过拟合 

- 卷积神经网络模拟人类的视觉处理流程,高效提取特征 
  - 可解决图片分类
  - 人类视觉：先看到小特征再逐渐拼接后反馈给人脑，这个过程非常短暂

## 分层

### 卷积层

- 卷积层使用多个卷积核对图像进行卷积操作
  - 卷积核：提取数字图片特征 
  - 卷积核有的叫作filter或者kernel
  - 为什么要用多个卷积核
    - 一个卷积核只能提取某一类特征 
- 多个卷积层逐层多次提取特征，最终输出一个值
  - 如：第一层提取横、竖，第二层提取组合后的直角
- 卷积层有权重需要训练，卷积核就是权重
  - 既：模型训练出卷积核大小

### 池化层

*属于优化层，不用也可以*

**作用：**

- 提取最强的特征
  - ![image-20201115205416825](https://tva1.sinaimg.cn/large/0081Kckwly1gkq5i1if1bj30pp0d0jwd.jpg)
- 扩大机器感受野
  - 图片上一个点的移动对人眼来说影响并不明显，但对计算就是不同的数据，所以可通过池化层扩大计算机的感受视野
-  减少计算量
  - 如上图：四个特征变一个特征了，一定程度防止过拟合（去除了不太强的特征）

**注意：**

- 池化层仅仅用于优化和压缩数据，没有权重需要训练
  - 所以池化层是固定的，算法也是固定的，一般是最大池化层

### 全连接层

**作用：**

- 作为输出层：和普通多层神经网络一样
- 作为分类器：解决分类问题

**注意：**

- 全连接层有权重需要训练

# 其他

## 静态服务器

npm i http-server -g

Hs data(文件夹名) --cors（解决跨域）

## 关于图片

784 = 28\*28\*1(图片是28像素，1代表黑白)  拥有784个像素值

如果是彩色图片就拥有rgb三个通道，这里就是3 

